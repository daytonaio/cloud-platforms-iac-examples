apiVersion: v1
kind: ServiceAccount
metadata:
  name: runtime-checker
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: runtime-checker
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: runtime-checker
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: runtime-checker
subjects:
  - kind: ServiceAccount
    name: runtime-checker
    namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: runtime-checker
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: runtime-checker
  template:
    metadata:
      labels:
        app: runtime-checker
    spec:
      serviceAccountName: runtime-checker
      priorityClassName: system-node-critical
      tolerations:
        - effect: NoSchedule
          operator: Exists
      nodeSelector:
        daytona.io/node-role: "workload"
      initContainers:
        - name: node-labeler
          image: bitnami/kubectl:latest
          command: ["/bin/sh", "-c"]
          args:
            - |-
              until kubectl get node $NODE_NAME -o jsonpath='{.status.nodeInfo.containerRuntimeVersion}' | grep -qE "cri-o"; do
                echo "Container runtime is not cri-o yet. Sleeping..."
                sleep 5
              done
              echo "Container runtime is now cri-o! Adding label to node..."
              kubectl label node $NODE_NAME --overwrite daytona.io/runtime-ready=true
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
      containers:
        - name: sleep
          image: registry.k8s.io/pause:3.1
